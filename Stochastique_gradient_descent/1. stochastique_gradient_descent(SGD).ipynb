{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd3d22f2-2635-4c59-a571-3db78e65fdc9",
   "metadata": {},
   "source": [
    "# Stochastique gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885e268-6c3a-428d-a932-37abc2c78674",
   "metadata": {},
   "source": [
    "In this lab , we will use the same objective in the lab{LInear regression}, so we will only change the bach gradient descent  with stochastique gradient descent.\n",
    "- **by bach** : use all the training examples for each calculation of the cost function\n",
    "- **stochastique** : use one training example randomly for each calculation of the cost fucntion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed215a-50c1-4020-8100-831ca43f2856",
   "metadata": {},
   "source": [
    "**WE should review the Linear regression lab to understand the content of this lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391afe3b-1ba2-4c5c-95c1-57db9e97384c",
   "metadata": {},
   "source": [
    "## 1. Importing the libiraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d91b454-d3fa-4713-b07a-e56002d3ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2700b-1733-442b-96d6-0e7cc354db13",
   "metadata": {},
   "source": [
    "## 2. The data set :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d894e-f920-4e5f-ae7e-0afb172af25f",
   "metadata": {},
   "source": [
    "we will use the data set houses.txt **{in github: Floder named DATASET > houses.txt}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b10db1-c4ea-4882-9025-25515c05b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(r\"C:\\Users\\hp\\Documents\\Apprentissage Automatique\\houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:, :4]\n",
    "    y = data[:, 4]\n",
    "    return X, y\n",
    "X_train, y_train = load_house_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff56f35-bacb-4b9d-9e48-db8ef7d2f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training X shape :  (99, 4)\n",
      "training Y shape:  (99,)\n"
     ]
    }
   ],
   "source": [
    "print('training X shape : ', X_train.shape)\n",
    "print('training Y shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "533acd4d-123f-4a49-a250-a00a8bd3634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. The cost function \n",
    "def compute_cost(X, y, w, b):\n",
    "    '''\n",
    "    X : the data set without the target variable\n",
    "    y:  the target variable\n",
    "    w:  the weight of the model\n",
    "    b : the intercept of the model \n",
    "    Return: the cost \n",
    "    '''\n",
    "    cost = 0.0\n",
    "    n = X.shape[0]\n",
    "    for i in range(n):\n",
    "        cost += ((np.dot(X[i], w) + b ) - y[i]) ** 2\n",
    "\n",
    "    cost /= (2*n)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5cd32-d267-4515-b5ac-e47873c41d7b",
   "metadata": {},
   "source": [
    "## 4. The compute gradient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb6af67-07dc-4022-9bbe-d0dc83617307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    '''\n",
    "    X : the data set without the target variable\n",
    "    y:  the target variable\n",
    "    w:  the weight of the model\n",
    "    b : the intercept of the model \n",
    "    Return: the cost \n",
    "    '''\n",
    "\n",
    "    n, m = X.shape\n",
    "    dw = np.zeros(m)\n",
    "    db = 0\n",
    "    # chose one training example randomly :\n",
    "    i = np.random.randint(n)\n",
    "    err = (np.dot(X[i], w) + b ) - y[i]\n",
    "\n",
    "    for j in range(m):\n",
    "        dw[j] = err * X[i][j]\n",
    "    db = err\n",
    "\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92343510-92e2-42ab-a43a-2e89486591e6",
   "metadata": {},
   "source": [
    "## 5. The gradient descente : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8018e47c-c2da-42b1-a931-b499e71e21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
    "    m = len(X)\n",
    "    hist = {}\n",
    "    hist[\"cost\"] = []\n",
    "    hist[\"params\"] = []\n",
    "    hist[\"grads\"] = []\n",
    "    hist[\"iter\"] = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    save_interval = np.ceil(num_iters / 10000)\n",
    "    print(\" Iteration             Cost     w0        w1       w2       w3      b        djdw0     djdw1   djdw2    djdw3     djdb \")\n",
    "    print(\"---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------\")\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        # Save cost J,w,b at each save interval for graphing\n",
    "        if i == 0 or i % save_interval == 0:\n",
    "            hist[\"cost\"].append(compute_cost(X, y, w, b))\n",
    "            hist[\"params\"].append([w, b])\n",
    "            hist[\"grads\"].append([dj_dw, dj_db])\n",
    "            hist[\"iter\"].append(i)\n",
    "    # Print cost every at intervals 10 times or as many iterations if <10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            cst = compute_cost(X, y, w, b)\n",
    "            print(f\"{i:9d} {cst:0.5e} {w[0]: 0.1e} {w[1]: 0.1e} {w[2]: 0.1e}{w[3]: 0.1e} {b: 0.1e} {dj_dw[0]: 0.1e} {dj_dw[1]: 0.1e} {dj_dw[2]: 0.1e}{dj_dw[3]: 0.1e} {dj_db: 0.1e}\")\n",
    "    return w, b, hist # return w,b and history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9ceb5-52f0-4216-87a9-cf872be85a5a",
   "metadata": {},
   "source": [
    "## 6. Run the gradient descent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c0fe2b2-0530-44ff-9091-eac38ed8a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stochastic_gradient_descent(X, y, iterations=1000, alpha=1e-6):\n",
    "    m, n = X.shape\n",
    "    # initialize parameters\n",
    "    initial_w = np.zeros(n)\n",
    "    initial_b = 0\n",
    "    # run stochastic gradient descent\n",
    "    w_out, b_out, hist_out = stochastic_gradient_descent(X, y, initial_w, initial_b, alpha, iterations)\n",
    "    print(f\"w,b found by stochastic gradient descent: w: {w_out}, b:{b_out:0.2f}\")\n",
    "    return w_out, b_out, hist_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd99e45a-ed13-44db-9e15-d37725db75a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration             Cost     w0        w1       w2       w3      b        djdw0     djdw1   djdw2    djdw3     djdb \n",
      "---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------\n",
      "        0 5.51505e+04  3.1e-02  9.0e-05  3.0e-05 1.3e-03  3.0e-05 -3.1e+05 -9.0e+02 -3.0e+02-1.3e+04 -3.0e+02\n",
      "    10000 1.83151e+03  2.8e-01 -3.7e-03 -7.3e-03-4.2e-01  5.6e-03 -7.1e+04 -9.1e+01 -4.6e+01-8.2e+02 -4.6e+01\n",
      "    20000 1.36765e+03  2.7e-01 -4.9e-03 -1.5e-02-6.0e-01  1.2e-02 -8.9e+04 -1.6e+02 -5.2e+01-6.8e+02 -5.2e+01\n",
      "    30000 1.51998e+03  2.8e-01 -5.1e-03 -2.2e-02-6.5e-01  2.0e-02  2.4e+04  4.5e+01  2.2e+01 2.2e+03  2.2e+01\n",
      "    40000 1.48329e+03  2.8e-01 -6.2e-03 -3.0e-02-7.1e-01  2.7e-02 -2.5e+04 -4.5e+01 -2.3e+01-1.4e+03 -2.3e+01\n",
      "    50000 1.33447e+03  2.7e-01 -6.5e-03 -3.7e-02-7.2e-01  3.5e-02  1.0e+04  2.1e+01  1.4e+01 1.1e+02  6.9e+00\n",
      "    60000 1.38119e+03  2.8e-01 -6.0e-03 -4.4e-02-7.2e-01  4.3e-02 -1.4e+05 -1.6e+02 -8.2e+01-1.7e+03 -8.2e+01\n",
      "    70000 1.46062e+03  2.6e-01 -5.9e-03 -5.1e-02-7.4e-01  5.0e-02  1.2e+05  2.0e+02  1.4e+02 5.7e+03  6.8e+01\n",
      "    80000 2.07752e+03  3.0e-01 -6.0e-03 -5.9e-02-7.4e-01  5.8e-02 -4.6e+04 -8.9e+01 -3.0e+01-4.1e+02 -3.0e+01\n",
      "    90000 1.33333e+03  2.7e-01 -6.3e-03 -6.6e-02-7.4e-01  6.6e-02 -1.2e+04 -1.9e+01 -1.9e+01-1.2e+02 -9.6e+00\n",
      "w,b found by stochastic gradient descent: w: [ 0.29591508 -0.00656004 -0.07335933 -0.72812885], b:0.07\n"
     ]
    }
   ],
   "source": [
    "#example : (the same result in the previous note book)\n",
    "w_out, b_out, hist_out = run_stochastic_gradient_descent(X_train, y_train, 100000,alpha=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d504c71-9864-41e9-9c93-9316ee4984ac",
   "metadata": {},
   "source": [
    "###### very simple --> the objective is to understand the stockastique gradient descent ........... by !!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
