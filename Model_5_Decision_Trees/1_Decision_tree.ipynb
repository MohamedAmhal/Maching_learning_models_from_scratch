{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c84be1a-1d4a-4609-97cd-f1dad8d914c7",
   "metadata": {},
   "source": [
    "# **Decison TREE** :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a1b08-a0aa-4bf9-a7cc-0964e6ae2ccd",
   "metadata": {},
   "source": [
    "### 1. Importing the libiraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c35ceb8-b0f8-4352-9101-73a05da9bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e50cb-252a-49f0-9639-9518a272aba5",
   "metadata": {},
   "source": [
    "### 2. The problem :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e99d1-4d13-45c5-889d-b768d1e6fa70",
   "metadata": {},
   "source": [
    "Suppose you're starting a business that grows and sells wild mushrooms.\n",
    "\n",
    "- Since not all mushrooms are safe to eat, you'd like to figure out if a mushroom is edible or poisonous based on its physical features.\n",
    "- You already have some data that can help with this.\n",
    "\n",
    "**Can you use this data to find out which mushrooms are safe to sell?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16104069-f012-4e59-962b-7f1b7a307b41",
   "metadata": {},
   "source": [
    "### 3. The data set :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd12f9-3841-43c6-b765-338cd9dda5d0",
   "metadata": {},
   "source": [
    "we have 10 training examples of mushrooms , so each training example we have 3 features :\n",
    "- **cap color** : this is the color of mushrooms (brown or red)\n",
    "- **stalk shape** : the shape of the legd (/\\ or \\/)\n",
    "- **Solitary** : is individual mushrooms or not (yes or no)\n",
    "- **Edible** : the target variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d2e57-af03-4c8d-96cd-0333281dedb5",
   "metadata": {},
   "source": [
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/y47h6FP/Capture-d-cran-2024-10-13-135850.png\" alt=\"Capture-d-cran-2024-10-13-135850\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5aa8f2-6d41-4d04-b130-d24fd3f74695",
   "metadata": {},
   "source": [
    "### 4. One-hot encoding :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb877929-2399-4221-b374-d2add7c0e899",
   "metadata": {},
   "source": [
    "one-hot encoding is a commonly used method to transform categorical variables into numerical variables. In this type of encoding, each unique category of a variable is converted into a new binary column, where a 1 indicates the presence of that category, and a 0 its absence.\n",
    "\n",
    "Here's how you can use Pandas' pd.get_dummies method to perform one-hot encoding on a DataFrame(we will not use it)==>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf72bb2-dc7a-4152-805b-c399428060a6",
   "metadata": {},
   "source": [
    "because we have a small data set so we will encode it manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830eef5-01f2-492f-912d-16c9d26577e7",
   "metadata": {},
   "source": [
    "X_train contains three features for each example:\n",
    "- Brown color (A value of 1 indicates a \"Brown\" cap color and 0 indicates a \"Red\" cap color).\n",
    "- Tapered shape (A value of 1 indicates a \"Tapered stem shape\" and 0 indicates a \"Broad stem shape\").\n",
    "- Solitary (A value of 1 indicates \"Yes\" and 0 indicates \"No\").\n",
    "\n",
    "\n",
    "y_train represents whether the mushroom is edible:\n",
    "- y = 1 indicates edible.\n",
    "- y = 0 indicates poisonous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd894afa-4839-42ac-83a9-3a5b9ae532fe",
   "metadata": {},
   "source": [
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/DgVCHmQ/aa.png\" alt=\"aa\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b650829-e2ab-4e80-b657-d9f43032b277",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dce451e-0a19-4700-bda3-bdc0a35f1853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First the 5 first elements of X_train:\n",
      " [[1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "Type of X_train: <class 'numpy.ndarray'>\n",
      "First the 5 first elements of y_train: [1 1 0 0 1]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First the 5 first elements of X_train:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))\n",
    "print(\"First the 5 first elements of y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40ccf73-34fa-41fe-904b-7ab43804333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (10, 3)\n",
      "The shape of y_train is:  (10,)\n",
      "Number of training examples (m): 10\n"
     ]
    }
   ],
   "source": [
    "#verify the shapes od the data set :\n",
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96c4ed-ae41-4516-b710-2b313e749314",
   "metadata": {},
   "source": [
    "### 5.decision tree Algorithm :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f673d-d809-4266-b29c-a402e341cc96",
   "metadata": {},
   "source": [
    "In this lab, you will build a decision tree based on the provided dataset.\n",
    "Remember that the steps to build a decision tree are as follows:\n",
    "\n",
    "- 1.Start with all the examples at the root node.\n",
    "- 2.Calculate the information gain for each possible split based on all the features, and choose the one that offers the highest information gain.\n",
    "- 3.Split the dataset based on the selected feature and create the left and right branches of the tree.\n",
    "- 4.Repeat the splitting process until the stopping criteria are met.\n",
    "\n",
    "\n",
    "In this lab, you will implement the following functions, which will allow you to split a node into left and right branches using the feature with the highest information gain:\n",
    "\n",
    "- Calculate entropy at a node.\n",
    "- Split the dataset at a node into left and right branches based on a given feature.\n",
    "- Calculate the information gain of a split based on a given feature.\n",
    "- Choose the feature that maximizes information gain\n",
    "\n",
    "\n",
    "Then, you will use the helper functions you've implemented to build a decision tree by repeating the splitting process until the stopping criteria are met. For this lab, the chosen stopping criterion is to set a maximum depth of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167ed51-9d42-4f9d-8b33-a9edadb9dcdc",
   "metadata": {},
   "source": [
    "#### **a. calculate the entropy :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8b89100-26de-46e1-8756-120c862e66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    '''\n",
    "    y : the data set contient the features\n",
    "    Return : the entropy\n",
    "    '''\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    else :\n",
    "        p1 = len([y[i] for i in range(len(y)) if y[i] == 1]) / len(y)\n",
    "        if p1 == 0 or p1 == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "            return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76ecd5c3-f1ea-4551-9fc7-eaf5d022fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root node:  1.0\n"
     ]
    }
   ],
   "source": [
    "# the entropy for this example is 1\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cafad-6485-47c0-bd29-0bbdc768563a",
   "metadata": {},
   "source": [
    "#### **b. spliting the data set at the left and right brsnches** :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe868b73-1277-4ab0-8d10-1de19c2c328d",
   "metadata": {},
   "source": [
    "Next, you will write a helper function called split_dataset that takes the data at a node and a feature to split on, then divides it into left and right branches. Later in this lab, you will implement code to calculate the quality of the split.\n",
    "\n",
    "The function takes as input the training data, a list of the indices of the data points at the node, and the feature on which you want to split.\n",
    "It divides the data and returns the subset of indices in the left and right branches.\n",
    "For example, suppose we start at the root node (so node_indices = [0,1,2,3,4,5,6,7,8,9]), and we choose to split based on feature 0, which is cap color (brown or not). The output of the function would then be left_indices = [0,1,2,3,4,7,9] (data points with brown caps) and right_indices = [5,6,8] (data points without brown caps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "045c65c7-4c52-4a8c-875e-14b0d97a9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, indices, I):\n",
    "    '''\n",
    "    X : the train data set\n",
    "    indices : the indices of trainig examples\n",
    "    I : the indice of the feature at the node \n",
    "    Return : the left indices and the right indices after spliting \n",
    "    '''\n",
    "    w_right = []\n",
    "    w_left = []\n",
    "    for i in indices:\n",
    "        if X[i][I] == 1:\n",
    "            w_left.append(i)\n",
    "        else:\n",
    "            w_right.append(i)\n",
    "\n",
    "\n",
    "    return w_left, w_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "726cdc77-ee32-4064-85d0-06908e24d7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "Right indices:  [5, 6, 8]\n",
      "CASE 2:\n",
      "Left indices:  [0, 2, 4]\n",
      "Right indices:  [6, 8]\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# Feel free to play around with these variables\n",
    "# The dataset only has three features, so this value can be 0 (Brown Cap),1 (Tapering Stalk Shape) or 2 (Solitary)\n",
    "feature = 0\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "print(\"CASE 1:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "# Case 2\n",
    "root_indices_subset = [0, 2, 4, 6, 8]\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices_subset,feature)\n",
    "print(\"CASE 2:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577e7b5-788b-4bc8-8786-b022dce4acee",
   "metadata": {},
   "source": [
    "#### **c.Calculating Information Gain**\r\n",
    "\r\n",
    "Next, you will write a function called `information_gain` that takes as input the training data, the indices at a node, and a feature to split on, then returns the information gain from the split.\r\n",
    "##\r\n",
    "## Information Gain Formula\r\n",
    "\r\n",
    "**Information Gain** = H(p<sub>node</sub>) - (w<sub>left</sub> × H(p<sub>left</sub>) + w<sub>right</sub> × H(p<sub>right</sub>##))\r\n",
    "\r\n",
    "## Explanation:\r\n",
    "\r\n",
    "- **H(p<sub>node</sub>)** is the entropy at the node.\r\n",
    "- **H(p<sub>left</sub>)** and **H(p<sub>right</sub>)** are the entropies of the left and right branches resulting from the split.\r\n",
    "- **w<sub>left</sub>** and **w<sub>right</sub>** are the proportions of examples in the left and right branches, respectively.\r\n",
    "\r\n",
    "**Note:** You can use the `compute_entropy()` function you implemented earlier to calculate the entropy.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4132cced-a44e-4430-ba3c-44bc34bbbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    '''\n",
    "    X : the train data set \n",
    "    y : the target variable\n",
    "    node_indices : the indices of the train set\n",
    "    feature : the feature that we will split the data set\n",
    "    Return : the infromation gain the cost\n",
    "    '''\n",
    "    Hp1_node = compute_entropy(y)\n",
    "    w_l, w_r = split_dataset(X, node_indices, feature)\n",
    "    y_l = [y[i] for i in w_l]\n",
    "    y_r = [y[i] for i in w_r]\n",
    "    Hp_left = compute_entropy(y_l)\n",
    "    Hp_right = compute_entropy(y_r)\n",
    "    w_left = len(w_l) / (len(w_l) + len(w_r))\n",
    "    w_right = len(w_r) / (len(w_l) + len(w_r))\n",
    "    gain = Hp1_node - (w_left * Hp_left + w_right * Hp_right)\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76f24e4d-2a1c-4207-9c87-f87fb44cfc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
      "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
      "Information Gain from splitting the root on solitary:  0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40280441-6e18-44ae-b08a-986a2ded314f",
   "metadata": {},
   "source": [
    "#### d.Finding the Best Split :\n",
    "\n",
    "Now, let's write a function to obtain the best feature to split on by calculating the information gain for each feature, as we did earlier, and returning the feature that provides the maximum information gain.\n",
    "\n",
    "The function is called `get_best_split()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f51e03e0-a574-44b8-9443-3036f143c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):\n",
    "    '''\n",
    "    X : the train set\n",
    "    y : this is the target varaible\n",
    "    node_indices : the indices of the data set \n",
    "    Return : best feature (the indice of the feature)\n",
    "    '''\n",
    "    features_number = len(X[0])\n",
    "    list_grain = {}\n",
    "    for i in range(features_number):\n",
    "        gain = compute_information_gain(X, y, node_indices, i)\n",
    "        list_grain[i] = gain\n",
    "    liste = list(list_grain.values())\n",
    "    maxi = max(liste)\n",
    "    f = -1\n",
    "    for j in list_grain.keys():\n",
    "        if list_grain[j] == maxi:\n",
    "            f = j\n",
    "            break\n",
    "\n",
    "    return f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbc303bc-9904-4275-bbfa-23d76a985867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n"
     ]
    }
   ],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f14994-3a9b-4587-b492-2e6f2bd23bd7",
   "metadata": {},
   "source": [
    "### 6. Implementing the decision tree algo : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1dee0295-329f-40c2-b09c-4a1fd955bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_recursive(X, y, node_indices, branche_name, max_depth, current_depth):\n",
    "    '''\n",
    "    X : the data set (train set)\n",
    "    y : the target variable\n",
    "    branche_name : the branche name of the tree ['Root', 'Left', 'Right']\n",
    "    max_depth : the maximum of the tree\n",
    "    current_depth : the current  level \n",
    "    Return : the decision tree \n",
    "    '''\n",
    "    if current_depth == max_depth :\n",
    "        formatting = \" \" * current_depth + \"-\" * current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branche_name, node_indices)\n",
    "        return\n",
    "    best_feature = get_best_split(X, y, node_indices)\n",
    "    formatting = \"-\" * current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branche_name, best_feature))\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "\n",
    "    build_tree_recursive(X=X, y=y, node_indices=left_indices, branche_name='Left', max_depth=max_depth, current_depth=(current_depth + 1))\n",
    "    build_tree_recursive(X=X, y=y, node_indices=right_indices, branche_name='Right', max_depth=max_depth, current_depth=current_depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "674a014e-890b-4258-acb9-9e465a6414d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [0, 1, 4, 7]\n",
      "  -- Right leaf node with indices [5]\n",
      "- Depth 1, Right: Split on feature: 1\n",
      "  -- Left leaf node with indices [8]\n",
      "  -- Right leaf node with indices [2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c4b4f-987c-418a-95f2-2c7e1483e79d",
   "metadata": {},
   "source": [
    "this is the implimentation of how the decision trees works "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22bcda-eba1-4ec6-b457-634bdf6f44fc",
   "metadata": {},
   "source": [
    "**in the next nchaalah we will create a class named DecisionTree that contient all function (predict , fit , score ...)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
